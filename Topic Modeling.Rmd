---
title: "Topic Modeling"
output: html_document
date: "2022-11-13"
author: "Qihan Su"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(topicmodels)
library(tidytext)
library(tidyverse)
library(tidytext)
library(ggplot2)
library(ggthemes)
# library(GGally)
# library(scales)
library(dplyr)
library(corrplot)
library(PerformanceAnalytics)
library(knitr)
library(stringr)
library(stringi)
library(tm)
# library(pacman)
library(tidyr)
library(tokenizers)
library(widyr)
library(igraph)
library(ggraph)
```





```{r}
IMDB<- read.csv( "/Users/suqihan/Desktop/IMDB Dataset.csv")
IMDB_df <- tibble(IMDB)
glimpse(IMDB_df)
head(IMDB_df)
IMDB_df %>% 
  mutate(review_number = row_number()) ->IMDB_df 
IMDB_df <- IMDB_df %>% select(-sentiment)
```

```{r}
IMDB <- IMDB  %>%  mutate(docs = c(1:length(IMDB$review)))

data(stop_words)
stop_words <- rbind(stop_words,c("br","Smart","movie","film","match","stop.oz","episode","series","season","5","version","10","1"))
```



###  LDA
```{r}

IMDB_dtm <- IMDB %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words)%>%
  count(docs, word) %>%
  cast_dtm(docs, word, n)
```

```{r}
ap_lda <- LDA(IMDB_dtm, k = 10, control = list(seed = 1234))

ap_topics <- tidy(ap_lda, matrix = "beta")

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)
ap_top_terms

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```
##As we can see from the gragh, ten themes can be extracted from the text.



```{r}
beta_wide_new <- head(beta_wide,10)
ggplot(data = beta_wide_new, aes(x = log_ratio, fill = term) ) + geom_histogram()
```

##Document classification
```{r}
ap_documents <- tidy(ap_lda, matrix = "gamma")
# ap_documents
ap_documents <- ap_documents %>%
  separate(document, c("title"),sep = "_", convert = TRUE)
ap_documents
```


```{r}
ggplot(ap_documents, aes(x = gamma , fill = as.factor(topic))) + geom_histogram()+
  facet_wrap(~topic, ncol = 3) + 
  scale_y_log10() +
  labs(title = "per-document-per-topic probabilities",
       y = "documents number",
       x= "gamma"
       )
```
##It can be seen that the topics of many documents are mixed and extracted from the two topics since they are all approximately to 50%
##From the ten chart above, we can see that the document 9 differs the most from other documents. In order to check this, we tidy the document 9 and to see the most common words in it. 

```{r}
ggplot(ap_documents, aes(factor(topic),gamma ))+ geom_boxplot()+
  labs(title = "per-document-per-topic probabilities",
       y = "gamma",
       x= "topic"
       )
```



##Tidy document 9
```{r}
tidy(IMDB_dtm) %>%
  filter(document == 9) %>%
  arrange(desc(count))
```


## word topic probability
```{r}
beta_wide <- ap_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic9 > .001 | topic3 > .001) %>%
  mutate(log_ratio = log2(topic9 / topic3))
beta_wide
```

##Comparing two topics' top ten frequncy words between topic 9 and 3
```{r}
tidy(IMDB_dtm) %>%
  filter(document == 9) %>%
  arrange(desc(count))
```
```{r}
tidy(IMDB_dtm) %>%
  filter(document == 3) %>%
  arrange(desc(count))
```
##From the ten 'gamma chart' above, we knew that the documents 9 differs the most, and it seems that document 2,3,5,7,8 are the most common documents so we select document 3 to comapre with 9. As we can see, the doucment 9 was talking about a nasty girl called Keitel, and document 3 was about comedy. 

##Classification in topic
```{r}
chapter_classifications <- ap_documents %>%
  group_by(topic) %>%
  slice_max(gamma) %>%
  ungroup()

chapter_classifications
```

##Analize word and document frequncy :tf-idf
```{r}
book_words <- IMDB %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words)%>%
  count(docs, word, sort = TRUE)

total_words <- book_words %>% 
  group_by(docs) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

freq_by_rank <- book_words %>% 
  group_by(docs) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total) %>%
  ungroup()

book_tf_idf <- book_words %>%
  bind_tf_idf(word, docs, n)

ap_lda <- LDA(book_tf_idf, k = 2, control = list(seed = 1234))
```


```{r}

word_pairs <- book_words %>% 
  pairwise_count(word,docs, sort = TRUE, upper = FALSE)
```

```{r}

set.seed(1234)
word_pairs %>%
  filter(n >= 4000) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```



##tokenizing by bigram
```{r}
token_bigram <- IMDB_df %>% unnest_tokens(bigram, words, token = "ngrams", n = 2) %>% 
  count(review,bigram,sort = TRUE)

# freq_bigram <- token_bigram %>% count(bigram,sort = TRUE)

total_bigram <- token_bigram %>% 
  group_by(review) %>% 
  summarize(total = sum(n))

review_bigram <- left_join(token_bigram, total_bigram)

#Zipfâ€™s law states that the frequency that a word appears is inversely proportional to its rank.
freq_by_rank_bi <- review_bigram %>% 
  group_by(review) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total) %>%
  ungroup()

#ggplot
freq_by_rank_bi %>% 
  ggplot(aes(rank, `term frequency`, color = review)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```


































```{r}
library(ggplot2)
library(dplyr)

top_terms <- topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```



