---
title: "Topic Modeling"
output: html_document
date: "2022-11-13"
author: "Qihan Su"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(topicmodels)
library(tidytext)
library(tidyverse)
library(tidytext)
library(ggplot2)
library(ggthemes)
# library(GGally)
# library(scales)
library(dplyr)
library(corrplot)
library(PerformanceAnalytics)
library(knitr)
library(stringr)
library(stringi)
library(tm)
# library(pacman)
library(tidyr)
library(tokenizers)
```





```{r}
IMDB<- read.csv( "/Users/suqihan/Desktop/IMDB Dataset.csv")
IMDB_df <- tibble(IMDB)
glimpse(IMDB_df)
head(IMDB_df)
IMDB_df %>% 
  mutate(review_number = row_number()) ->IMDB_df 
IMDB_df <- IMDB_df %>% select(-sentiment)
```

```{r}
IMDB <- IMDB  %>%  mutate(docs = c(1:length(IMDB$review)))

data(stop_words)
stop_words <- rbind(stop_words,c("br","Smart"))
```



###  LDA
```{r}

IMDB_dtm <- IMDB %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words)%>%
  count(docs, word) %>%
  cast_dtm(docs, word, n)
```

```{r}
ap_lda <- LDA(IMDB_dtm, k = 10, control = list(seed = 1234))

ap_topics <- tidy(ap_lda, matrix = "beta")

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)
ap_top_terms

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```
##As we can see from the gragh, ten themes can be extracted from the text.



```{r}
beta_wide_new <- head(beta_wide,10)
ggplot(data = beta_wide_new, aes(x = log_ratio, fill = term) ) + geom_histogram()
```

##Document classification
```{r}
ap_documents <- tidy(ap_lda, matrix = "gamma")
# ap_documents
ap_documents <- ap_documents %>%
  separate(document, c("title"),sep = "_", convert = TRUE)
ap_documents
```


```{r}
ggplot(ap_documents, aes(x = gamma , fill = as.factor(topic))) + geom_histogram()+
  facet_wrap(~topic, ncol = 3) + 
  scale_y_log10() +
  labs(title = "per-document-per-topic probabilities",
       y = "documents number",
       x= "gamma"
       )
```
##It can be seen that the topics of many documents are mixed and extracted from the two topics since they are all approximately to 50%
##From the ten chart above, we can see that the document 9 differs the most from other documents. In order to check this, we tidy the document 9 and to see the most common words in it. 

```{r}
ggplot(ap_documents, aes(factor(topic),gamma ))+ geom_boxplot()+
  labs(title = "per-document-per-topic probabilities",
       y = "gamma",
       x= "topic"
       )
```



##Tidy document 9
```{r}
tidy(IMDB_dtm) %>%
  filter(document == 9) %>%
  arrange(desc(count))
```


## word topic probability
```{r}
beta_wide <- ap_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic9 > .001 | topic3 > .001) %>%
  mutate(log_ratio = log2(topic9 / topic3))
beta_wide
```

##Comparing two topics' top ten frequncy words between topic 9 and 3
```{r}
tidy(IMDB_dtm) %>%
  filter(document == 9) %>%
  arrange(desc(count))
```
```{r}
tidy(IMDB_dtm) %>%
  filter(document == 3) %>%
  arrange(desc(count))
```
##From the ten 'gamma chart' above, we knew that the documents 9 differs the most, and it seems that document 2,3,5,7,8 are the most common documents so we select document 3 to comapre with 9. As we can see, the doucment 9 was talking about a nasty girl called Keitel, and document 3 was about comedy. 


##tokenizing by bigram
```{r}
token_bigram <- IMDB_df %>% unnest_tokens(bigram, words, token = "ngrams", n = 2) %>% 
  count(review,bigram,sort = TRUE)

# freq_bigram <- token_bigram %>% count(bigram,sort = TRUE)

total_bigram <- token_bigram %>% 
  group_by(review) %>% 
  summarize(total = sum(n))

review_bigram <- left_join(token_bigram, total_bigram)

#Zipfâ€™s law states that the frequency that a word appears is inversely proportional to its rank.
freq_by_rank_bi <- review_bigram %>% 
  group_by(review) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total) %>%
  ungroup()

#ggplot
freq_by_rank_bi %>% 
  ggplot(aes(rank, `term frequency`, color = review)) + 
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  scale_x_log10() +
  scale_y_log10()
```

##tf-idf
```{r}
review_tf_idf_bi <- review_bigram %>%
  bind_tf_idf(bigram, review, n)

#look at terms with high tf-idf in reviews.
review_tf_idf_bi %>%
  select(-total) %>%
  arrange(desc(tf_idf))

#too many 'br br' -> let's remove 'br br'
review_separated <- review_bigram %>%  
  separate(bigram, into = c("word1", "word2"), sep = " ")

review_united <- review_separated %>%
  filter(!word1 %in% c('br'),
         !word2 %in% c('br')) %>%
  unite(bigram, c(word1, word2), sep = " ")

#resume tf-idf
review_tf_idf_bi2 <- review_united %>%
  bind_tf_idf(bigram, review, n)

review_tf_idf_bi2 

#look at terms with high tf-idf in reviews.
review_tf_idf_bi2 %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```

```{r}
review_tf_idf_bi2 %>%
  group_by(review) %>%
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(bigram, tf_idf), fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~review, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)


words_tf_idf <- review_united %>%
  review_tf_idf_bi2
  group_by(review) %>% 
  top_n(10) %>%
  ungroup() %>% 
  facet_bar(y = bigram, x = tf_idf, by = review, ncol = 3) + 
  labs(title = "Top 10 words picked tf-idf")

```




















```{r}
library(ggplot2)
library(dplyr)

top_terms <- topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```



